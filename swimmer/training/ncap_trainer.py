#!/usr/bin/env python3
"""
NCAP Trainer implementation.
"""

import torch
import torch.nn as nn
import numpy as np
import os
import tonic
import tonic.torch
from ..environments.mixed_environment import MixedSwimmerEnv
from ..environments.tonic_wrapper import TonicSwimmerWrapper
from ..models.ncap_swimmer import NCAPSwimmer
from ..models.tonic_ncap import create_tonic_ncap_model
from ..utils.training_logger import TrainingLogger
from .swimmer_trainer import SwimmerTrainer


class NCAPTrainer(SwimmerTrainer):
    """
    Improved trainer specifically designed for NCAP model stability.
    Addresses numerical instability and parameter corruption issues.
    """
    
    def __init__(
        self,
        n_links=6,
        algorithm='a2c',
        training_steps=50000,
        save_steps=10000,
        output_dir=None,
        log_dir='results/manual_run',
        log_episodes=5,
        sparse_init: bool = False,
        sparse_reg_lambda: float = 0.0,
        force_oscillation: bool = False,
    ):
        # Use smaller training steps for more controlled training
        super().__init__(
            model_type='ncap',
            algorithm=algorithm,
            n_links=n_links,
            training_steps=training_steps,
            save_steps=save_steps,
            output_dir=output_dir,
            log_dir=log_dir,
            log_episodes=log_episodes,
            sparse_init=sparse_init,
            sparse_reg_lambda=sparse_reg_lambda,
            force_oscillation=force_oscillation,
        )
        
        # NCAP-specific training parameters - optimized for performance
        self.ncap_learning_rate = 3e-4  # Higher learning rate for faster learning
        self.ncap_gradient_clip = 0.5   # Standard gradient clipping
        self.parameter_constraint_strength = 0.05  # Lighter regularization for better performance
        self.stability_check_frequency = 100  # Less frequent checks for faster training
        
        # Early stopping parameters
        self.early_stopping_patience = 30  # allow longer learning before stopping
        self.min_improvement_threshold = 0.01  # Minimum improvement to reset patience
        self.early_stopping_metric = 'reward'  # Track reward for improvement
        
        # Training monitoring
        self.training_metrics = {
            'nan_detections': 0,
            'parameter_resets': 0,
            'gradient_clips': 0,
            'oscillator_drift': [],
            'interval_rewards': [],  # Track rewards per interval for early stopping
            'best_reward': float('-inf'),
            'patience_counter': 0,
            'viscosities': [],   # log sampled viscosity per interval evaluation
            'visc_rewards': []   # corresponding rewards
        }
        
    def create_base_ncap_model(self, n_joints):
        """Create NCAP model with stable initialization for training."""
        model = NCAPSwimmer(n_joints=n_joints, oscillator_period=60, memory_size=10)
        
        # Apply improved initialization
        self._apply_stable_initialization(model)
        
        model.to(self.device)
        return model

    def create_tonic_ncap_model(self, n_joints):
        """Create Tonic-compatible NCAP model with improved initialization."""
        self._prepare_sparse_priors(num_segments=n_joints)
        model = create_tonic_ncap_model(
            n_joints=n_joints, 
            oscillator_period=60, 
            memory_size=10,
            action_noise=0.05,  # Smaller noise for stability
            num_segments=max(1, int(n_joints)),
            prior_modulation_scale=self.prior_modulation_scale,
        )

        # Legacy stability initialization is incompatible with explicit
        # sparse-prior/tabula-rasa initialization modes; keep model defaults.
        
        model.to(self.device)
        return model
    
    def _apply_stable_initialization(self, model):
        """Apply initialization for good swimming performance and stability."""
        with torch.no_grad():
            # Initialize NCAP parameters for good swimming performance
            for name, param in model.params.items():
                if 'muscle' in name or 'bneuron' in name:
                    # Initialize for good swimming: higher values promote stronger oscillations
                    nn.init.normal_(param, mean=1.0, std=0.3)  # Higher mean for effective swimming
                    param.data.clamp_(0.2, 3.0)  # Allow stronger parameters for better performance
            
            # Initialize environment adaptation modules
            if hasattr(model, 'env_modulation'):
                nn.init.xavier_uniform_(model.env_modulation.weight, gain=0.2)  # Moderate adaptation strength
                nn.init.constant_(model.env_modulation.bias, 0.0)
            
            if hasattr(model, 'amplitude_modulation'):
                nn.init.xavier_uniform_(model.amplitude_modulation.weight, gain=0.2)
                nn.init.constant_(model.amplitude_modulation.bias, 0.0)
            
            if hasattr(model, 'memory_decoder'):
                nn.init.xavier_uniform_(model.memory_decoder.weight, gain=0.2)
                nn.init.constant_(model.memory_decoder.bias, 0.0)
            
            # Initialize LSTM for environment memory
            if hasattr(model, 'memory_encoder'):
                for name, param in model.memory_encoder.named_parameters():
                    if 'weight' in name:
                        nn.init.xavier_uniform_(param, gain=0.5)  # Good initialization for LSTM
                    elif 'bias' in name:
                        nn.init.constant_(param, 0.0)
    
    def create_stable_tonic_agent(self, model):
        """Create PPO/A2C agent optimized for adaptive locomotion learning."""
        from tonic.torch import updaters
        from .custom_tonic_agent import CustomA2C, CustomPPO
        
        if self.algorithm == 'ppo':
            actor_updater = updaters.ClippedRatio(
                optimizer=lambda params: torch.optim.Adam(params, lr=3e-4),
                ratio_clip=0.2,
                kl_threshold=0.015,
                entropy_coeff=0.05,
                gradient_clip=0.5,
            )
            agent_cls = CustomPPO
        elif self.algorithm == 'a2c':
            actor_updater = updaters.StochasticPolicyGradient(
                optimizer=lambda params: torch.optim.Adam(params, lr=3e-4),
                entropy_coeff=0.05,
                gradient_clip=0.5,
            )
            agent_cls = CustomA2C
        else:
            raise ValueError(f"Unsupported algorithm: {self.algorithm}")
        
        critic_updater = updaters.VRegression(
            optimizer=lambda params: torch.optim.Adam(params, lr=6e-4),
            gradient_clip=0.5
        )
        
        # Larger replay buffer
        from tonic import replays
        replay = replays.Segment(size=8192, batch_iterations=64)
        
        return agent_cls(
            model=model,
            replay=replay,
            actor_updater=actor_updater,
            critic_updater=critic_updater,
            prior_reg_lambda=self.effective_prior_lambda,
            force_oscillation=self.force_oscillation,
        )
    
    def monitor_parameter_stability(self, model):
        """Monitor NCAP parameters for stability during training."""
        with torch.no_grad():
            # Check for NaN values in key parameters
            nan_detected = False
            
            # Check NCAP biological parameters for NaN
            for name, param in model.ncap.params.items():
                if torch.isnan(param).any():
                    print(f"WARNING: NaN detected in {name}")
                    nan_detected = True
            
            # Check environment adaptation modules
            if hasattr(model.ncap, 'env_modulation'):
                if torch.isnan(model.ncap.env_modulation.weight).any():
                    print("WARNING: NaN detected in env_modulation weights")
                    nan_detected = True
            
            # Check for parameter drift in biological parameters
            for name, param in model.ncap.params.items():
                param_value = param.item() if param.numel() == 1 else param.mean().item()
                if param_value > 10.0 or param_value < -10.0:  # Reasonable biological range
                    print(f"WARNING: Parameter drift detected in {name}: {param_value}")
                    self.training_metrics['oscillator_drift'].append(param_value)
            
            if nan_detected:
                self.training_metrics['nan_detections'] += 1
                self._reset_problematic_parameters(model)
                return False
            
            return True
    
    def _reset_problematic_parameters(self, model):
        """Reset parameters that have become unstable."""
        print("Resetting problematic parameters to stable values")
        with torch.no_grad():
            # Reset NCAP biological parameters if they have NaN values
            for name, param in model.ncap.params.items():
                if torch.isnan(param).any():
                    print(f"Resetting {name} parameter")
                    if 'muscle' in name or 'bneuron' in name:
                        # Reset to stable biological values
                        param.data.fill_(0.5)
                        param.data.clamp_(0.1, 2.0)
            
            # Reset environment adaptation modules if needed
            if hasattr(model.ncap, 'env_modulation'):
                if torch.isnan(model.ncap.env_modulation.weight).any():
                    print("Resetting env_modulation weights")
                    nn.init.xavier_uniform_(model.ncap.env_modulation.weight, gain=0.01)
                    nn.init.constant_(model.ncap.env_modulation.bias, 0.0)
        
        self.training_metrics['parameter_resets'] += 1
    
    def apply_parameter_constraints(self, model):
        """Apply constraints to keep parameters within reasonable ranges while allowing good performance."""
        with torch.no_grad():
            # Constrain NCAP biological parameters to effective ranges
            for name, param in model.ncap.params.items():
                if 'muscle' in name or 'bneuron' in name:
                    # Allow stronger parameters for effective swimming
                    param.data.clamp_(0.1, 5.0)  # Expanded range for better performance
            
            # Constrain environment adaptation modules
            if hasattr(model.ncap, 'env_modulation'):
                model.ncap.env_modulation.weight.data.clamp_(-2.0, 2.0)  # Allow stronger adaptation
            
            if hasattr(model.ncap, 'amplitude_modulation'):
                model.ncap.amplitude_modulation.weight.data.clamp_(-1.0, 1.0)  # Allow amplitude modulation
    
    def compute_parameter_regularization_loss(self, model):
        """Compute light regularization loss to maintain biological plausibility."""
        # Target values for biological parameters (good swimming defaults)
        target_value = 1.0  # Higher target for good swimming performance
        
        # Compute L2 regularization for NCAP biological parameters
        total_reg = 0.0
        param_count = 0
        
        for name, param in model.ncap.params.items():
            if 'muscle' in name or 'bneuron' in name:
                reg_loss = (param - target_value) ** 2
                total_reg += reg_loss.sum()
                param_count += param.numel()
        
        # Average regularization loss
        if param_count > 0:
            total_reg = total_reg / param_count
        
        return self.parameter_constraint_strength * total_reg
    
    def train_with_improved_stability(self):
        """Train using improved stability measures with a proven approach."""
        from datetime import datetime
        start_ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{start_ts}] ‚ñ∂ Starting base NCAP training with enhanced stability measures")

        # Set up Tonic logger in the run-local artifact directory.
        tonic.logger.initialize(path=self.tonic_log_dir)
        
        # Create environment
        env = self.create_tonic_environment()
        n_joints = env.action_space.shape[0]
        
        # Create improved NCAP model
        tonic_model = self.create_tonic_ncap_model(n_joints)
        
        # Create stable agent
        agent = self.create_stable_tonic_agent(tonic_model)
        
        # Initialize agent
        agent.initialize(
            observation_space=env.observation_space,
            action_space=env.action_space,
            seed=42
        )
        
        # Use proven training approach with intervals and monitoring
        self._run_interval_training(agent, env, tonic_model)
        
        # Save final model
        self.save_tonic_model(agent, f"base_ncap_{self.n_links}links")
        
        # Store for evaluation
        self.agent = agent
        self.env = env
        self.model = tonic_model
        
        # Print training metrics
        self._print_training_metrics()

        # Plot interval reward progression
        from ..utils.visualization import plot_training_interval_rewards
        from ..utils.visualization import plot_reward_vs_viscosity
        interval_plot = os.path.join(self.training_log_dir, f'interval_rewards_{self.n_links}links.png')
        plot_training_interval_rewards(self.training_metrics['interval_rewards'], interval_plot)
        print(f"Interval reward plot saved to {interval_plot}")

        # Scatter reward vs viscosity
        scatter_path = os.path.join(self.training_log_dir, f'reward_vs_viscosity_{self.n_links}links.png')
        plot_reward_vs_viscosity(self.training_metrics['viscosities'], self.training_metrics['visc_rewards'], scatter_path)
        print(f"Reward-vs-viscosity plot saved to {scatter_path}")

        # --- Automatic mixed-environment evaluation ---
        print("\nRunning post-training evaluation ‚Ä¶")
        eval_results = self.evaluate_mixed_environment(max_frames=5000, speed_factor=1.0)
        print(f"Distance travelled post-training: {eval_results['total_distance']:.3f} m")
        
        end_ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{end_ts}] ‚úî Base NCAP training completed and evaluated!")
    
    def _run_interval_training(self, agent, env, model):
        """Run training in intervals with stability checks and early stopping."""
        print(f"Running interval training for {self.training_steps} total steps...")
        print(f"Early stopping: patience={self.early_stopping_patience} intervals, "
              f"min_improvement={self.min_improvement_threshold}")
        
        # Use longer intervals for better learning while maintaining monitoring
        interval_size = min(2000, self.training_steps // 2)  # Longer training chunks
        completed_steps = 0
        
        while completed_steps < self.training_steps:
            remaining_steps = self.training_steps - completed_steps
            current_interval = min(interval_size, remaining_steps)
            
            print(f"\nTraining interval: {completed_steps} to {completed_steps + current_interval}")
            
            # Create a mini-trainer for this interval
            from tonic import Trainer
            trainer = Trainer(
                steps=current_interval,
                save_steps=current_interval + 1,  # Don't save during interval
                test_episodes=0  # Skip testing during training
            )
            
            trainer.initialize(
                agent=agent,
                environment=env,
                test_environment=env
            )
            
            # Check stability before training (less frequent)
            if completed_steps % (interval_size * 2) == 0:  # Check every 2nd interval
                pre_stable = self.monitor_parameter_stability(model)
                if not pre_stable:
                    print("Applying stability corrections before interval...")
                    self.apply_parameter_constraints(model)
            
            # Run the training interval
            try:
                trainer.run()
                print(f"‚úÖ Interval {completed_steps}-{completed_steps + current_interval} completed")
            except Exception as e:
                print(f"‚ö†Ô∏è Training interval had issues: {e}")
                self.apply_parameter_constraints(model)
            
            # **EARLY STOPPING CHECK**
            interval_reward = self._evaluate_interval_performance(agent, env)
            self.training_metrics['interval_rewards'].append(interval_reward)

            # Log viscosity --> reward pair for scatter plot (must come after interval_reward is defined)
            try:
                current_visc = env.env.task.current_viscosity
                self.training_metrics['viscosities'].append(current_visc)
                self.training_metrics['visc_rewards'].append(interval_reward)
            except Exception:
                pass
            
            # Check for improvement
            if interval_reward > self.training_metrics['best_reward'] + self.min_improvement_threshold:
                print(f"üìà Performance improved: {interval_reward:.3f} (prev best: {self.training_metrics['best_reward']:.3f})")
                self.training_metrics['best_reward'] = interval_reward
                self.training_metrics['patience_counter'] = 0  # Reset patience
            else:
                self.training_metrics['patience_counter'] += 1
                print(f"üìâ No improvement: {interval_reward:.3f} (patience: {self.training_metrics['patience_counter']}/{self.early_stopping_patience})")
            
            # Early stopping decision
            if self.training_metrics['patience_counter'] >= self.early_stopping_patience:
                print(f"\nüõë EARLY STOPPING TRIGGERED!")
                print(f"   No improvement for {self.early_stopping_patience} intervals")
                print(f"   Best reward achieved: {self.training_metrics['best_reward']:.3f}")
                print(f"   Steps completed: {completed_steps + current_interval}")
                break
            
            # Check stability after training (less frequent)
            if (completed_steps + current_interval) % (interval_size * 2) == 0:
                post_stable = self.monitor_parameter_stability(model)
                if not post_stable:
                    print("Applying stability corrections after interval...")
                    self.apply_parameter_constraints(model)
            
            # Light constraints after each interval (not heavy-handed)
            if completed_steps % interval_size == 0:
                self.apply_parameter_constraints(model)
            
            # Log progress - show biological parameter statistics
            param_stats = []
            for name, param in model.ncap.params.items():
                param_value = param.item() if param.numel() == 1 else param.mean().item()
                param_stats.append(f"{name}={param_value:.3f}")
            print(f"Post-interval parameters: {', '.join(param_stats[:3])}...")  # Show first 3
            
            completed_steps += current_interval
        
        if self.training_metrics['patience_counter'] < self.early_stopping_patience:
            print(f"\nTraining completed: {completed_steps} total steps")
        else:
            print(f"\nTraining stopped early: {completed_steps} total steps")
        
        # Final stability check
        print("Performing final stability check...")
        final_stable = self.monitor_parameter_stability(model)
        self.apply_parameter_constraints(model)
        
        # Show final parameter statistics
        param_stats = []
        for name, param in model.ncap.params.items():
            param_value = param.item() if param.numel() == 1 else param.mean().item()
            param_stats.append(f"{name}={param_value:.3f}")
        print(f"Final parameters: {', '.join(param_stats[:5])}...")  # Show first 5
        print(f"Final stability: {'‚úÖ STABLE' if final_stable else '‚ö†Ô∏è UNSTABLE (corrected)'}")
    
    def _evaluate_interval_performance(self, agent, env, num_episodes=3):
        """Quick evaluation to check training progress for early stopping."""
        total_reward = 0
        
        for _ in range(num_episodes):
            obs = env.reset()
            episode_reward = 0
            steps = 0
            max_steps = 200  # Short evaluation
            
            while steps < max_steps:
                action = agent.test_step(obs, steps=steps)
                if torch.is_tensor(action):
                    action = action.cpu().numpy()
                
                obs, infos = env.step(action)
                reward = infos['rewards'][0] if 'rewards' in infos else 0
                episode_reward += reward
                steps += 1
                
                if infos.get('resets', [False])[0]:
                    break
            
            total_reward += episode_reward
        
        avg_reward = total_reward / num_episodes
        return avg_reward
    
    def _print_training_metrics(self):
        """Print training stability metrics and early stopping information."""
        print("\n=== TRAINING STABILITY METRICS ===")
        print(f"NaN detections: {self.training_metrics['nan_detections']}")
        print(f"Parameter resets: {self.training_metrics['parameter_resets']}")
        print(f"Gradient clips: {self.training_metrics['gradient_clips']}")
        
        print("\n=== EARLY STOPPING METRICS ===")
        print(f"Intervals completed: {len(self.training_metrics['interval_rewards'])}")
        print(f"Best reward achieved: {self.training_metrics['best_reward']:.3f}")
        print(f"Final patience counter: {self.training_metrics['patience_counter']}")
        
        if self.training_metrics['interval_rewards']:
            rewards = self.training_metrics['interval_rewards']
            print(f"Reward progression: {[f'{r:.2f}' for r in rewards[-5:]]}")  # Show last 5
            
            if len(rewards) >= 2:
                trend = "improving" if rewards[-1] > rewards[0] else "declining"
                print(f"Overall trend: {trend} ({rewards[0]:.2f} ‚Üí {rewards[-1]:.2f})")
        
        early_stopped = self.training_metrics['patience_counter'] >= self.early_stopping_patience
        print(f"Training outcome: {'üõë Early stopped' if early_stopped else '‚úÖ Completed'}")
    
    def train(self):
        """Override parent train method to use improved stability training."""
        self.train_with_improved_stability()
    
    def compare_with_default_ncap(self):
        """Compare trained model performance with default NCAP."""
        if self.model is None:
            raise ValueError("No trained model available. Train first.")
        
        print("\n=== TRAINED MODEL PERFORMANCE ===")
        trained_results = self.evaluate_mixed_environment(max_frames=5000)

        # Determine success without baseline comparison (baseline removed)
        success = (
            trained_results['total_distance'] >= 1.0 and
            trained_results['avg_velocity'] >= 0.03
        )
        adaptive_bonus = trained_results['env_transitions'] >= 1

        print(f"Distance traveled: {trained_results['total_distance']:.3f} m")
        print(f"Average velocity : {trained_results['avg_velocity']:.3f} m/s")
        print(f"Environment transitions: {trained_results['env_transitions']}")
        print(f"\nTraining Success: {'‚úÖ YES' if success else '‚ùå NO'}")
        
        return {
            'success': success,
            'forward_locomotion': success,
            'adaptive_bonus': adaptive_bonus,
            'trained_results': trained_results,
            'default_results': None
        } 

    def create_simple_tonic_environment(self):
        """Create simple swimmer environment lazily (only if method used)."""
        from ..environments.simple_swimmer import TonicSimpleSwimmerWrapper  # local import to avoid hard dependency
        return TonicSimpleSwimmerWrapper(n_links=self.n_links, time_feature=True, desired_speed=0.1)
    
    def train_simple_swimming(self):
        """Train using simple forward swimming environment with all stability fixes."""
        print("Starting improved NCAP training on SIMPLE SWIMMING environment")
        print("This uses all stability fixes but focuses on basic forward movement")
        
        # Set up Tonic logger
        tonic.logger.initialize(path=os.path.join(self.tonic_log_dir, "simple_swimming"))
        
        # Create SIMPLE environment instead of mixed
        env = self.create_simple_tonic_environment()
        n_joints = env.action_space.shape[0]
        
        print(f"Environment: {env.name} (simple forward swimming)")
        print(f"Observation space: {env.observation_space.shape}")
        print(f"Action space: {env.action_space.shape}")
        
        # Create improved NCAP model (same stability fixes)
        tonic_model = self.create_tonic_ncap_model(n_joints)
        
        # Create stable agent (same configuration)
        agent = self.create_stable_tonic_agent(tonic_model)
        
        # Initialize agent
        agent.initialize(
            observation_space=env.observation_space,
            action_space=env.action_space,
            seed=42
        )
        
        # Use proven training approach with intervals and monitoring
        self._run_interval_training(agent, env, tonic_model)
        
        # Save final model
        self.save_tonic_model(agent, f"simple_ncap_{self.n_links}links")
        
        # Store for evaluation
        self.agent = agent
        self.env = env
        self.model = tonic_model
        
        # Print training metrics
        self._print_training_metrics()
        
        print("Simple swimming NCAP training completed!")
        return agent, env, tonic_model
    
    def evaluate_simple_swimming(self):
        """Evaluate the trained model on simple swimming performance."""
        if self.model is None or self.agent is None or self.env is None:
            raise ValueError("No trained model available. Train first using train_simple_swimming().")
        
        print("=== EVALUATING SIMPLE SWIMMING PERFORMANCE ===")
        
        num_episodes = 5
        total_rewards = []
        total_distances = []
        total_velocities = []
        
        for episode in range(num_episodes):
            obs = self.env.reset()
            episode_reward = 0
            episode_length = 0
            max_steps = 1000
            
            initial_pos = None
            velocities = []
            
            while episode_length < max_steps:
                # Get action from trained agent
                action = self.agent.test_step(obs, steps=episode_length)
                if torch.is_tensor(action):
                    action = action.cpu().numpy()
                
                # Take step
                obs, infos = self.env.step(action)
                reward = infos['rewards'][0]
                done = infos['resets'][0]
                
                episode_reward += reward
                episode_length += 1
                
                # Track distance and velocity (simple environment doesn't have physics access)
                # We'll estimate from rewards and steps
                velocities.append(abs(reward))  # Approximate velocity from reward
                
                if done:
                    break
            
            # Calculate episode metrics
            episode_distance = episode_length * 0.001  # Rough distance estimate
            avg_velocity = np.mean(velocities) if velocities else 0.0
            
            total_rewards.append(episode_reward)
            total_distances.append(episode_distance)
            total_velocities.append(avg_velocity)
            
            print(f"  Episode {episode + 1}: Reward={episode_reward:.2f}, "
                  f"Distance={episode_distance:.3f}, Velocity={avg_velocity:.3f}")
        
        # Calculate final metrics
        avg_reward = np.mean(total_rewards)
        avg_distance = np.mean(total_distances)
        avg_velocity = np.mean(total_velocities)
        
        print(f"\n=== SIMPLE SWIMMING RESULTS ===")
        print(f"Average Reward: {avg_reward:.2f}")
        print(f"Average Distance: {avg_distance:.3f}")
        print(f"Average Velocity: {avg_velocity:.3f}")
        
        # Success criteria for forward movement
        success = avg_distance >= 0.5 and avg_velocity >= 0.02  # Relaxed criteria
        print(f"Forward Movement Success: {'‚úÖ YES' if success else '‚ùå NO'}")
        
        return {
            'avg_reward': avg_reward,
            'avg_distance': avg_distance,
            'avg_velocity': avg_velocity,
            'success': success,
            'rewards': total_rewards,
            'distances': total_distances,
            'velocities': total_velocities
        } 
